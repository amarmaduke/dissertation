\chapter{Introduction}
\input{figures/01/typing_macros.tex}
\input{figures/01/reduction_macros.tex}

Type theory is a tool for reasoning about assertions of some domain of discourse.
When applied to programming languages, that domain is the expressible programs and their properties.
Of course, a type theory may be rich enough to express detailed properties about a program, such that it halts or returns an even number.
Therein lies a tension between what properties a type theory can faithfully (i.e. consistently) encode and the complexity of the type theory itself.
If the theory is too complex then it may be untenable to prove that the type theory is well-behaved.
Indeed, the design space of type theories is vast, likely infinite.
When incorporating features the designer must balance complexity against capability.

Modern type theory arguably began with Martin-L\"{o}f in the 1970s and 1980s when he introduced a dependent type theory with the philosophical aspirations of being an alternative foundation of mathematics \cite{lof1975,lof1984}.
Soon after in 1985, the Calculus of Constructions (CC) was introduced by Coquand \cite{coquand1985,coquand1986}.
Inductive data (e.g. natural numbers, lists, trees) was shown by Guevers to be impossible to derive in CC \cite{geuvers2001_noind}.
Nevertheless, inductive data was added as an extension by Pfenning \cite{pfenning1989} and the Calculus of Inductive Constructions (CIC) became the basis for the proof assistant Rocq \cite{paulin-mohring1993}.

In the early 1990s Barendregt introduced a generalization to Pure Type Systems (PTS) and studied CC under his now famous $\lambda$-cube \cite{barendregt1990_cube,barendregt1991_pts}.
The $\lambda$-cube demonstrated how CC could be deconstructed into four essential sorts of functions.
At its base was the Simply Typed Lambda Calculus (STLC) a type theory introduced in the 1940s by Church to correct logical consistency issues in his (untyped) $\lambda$-calculus \cite{church1940_stlc}.
The STLC has only basic functions found in all programming languages.
System F, a type theory introduced by Girard \cite{girard1972,girard1989} and independently by Reynolds \cite{reynolds1974_systemf}, is obtained from STLC by adding quantification over types (i.e. polymorphic functions).
Adding a copy of STLC at the type-layer, functions from types to types, yields System F$^\omega$.
Finally, the addition of quantification over terms or functions from terms to types, completes CC.
While this is not the only path through the $\lambda$-cube to arrive at CC it is the most well-known and the most immediately relevant.

Perhaps surprisingly, all the systems of the $\lambda$-cube correspond to a logic.
In the 1970s Curry circulated his observations about the STLC corresponding to intuitionistic propositional logic \cite{howard1980}.
Reynolds and Girard's combined work demonstrated that System F corresponds to second-order intuitionistic propositional logic \cite{girard1972,reynolds1974_systemf,reynolds1983}.
Indeed, Barendregt extended the correspondence to all systems in his $\lambda$-cube noting System F$^\omega$ as corresponding to higher-order intuitionistic propositional logic and CC as corresponding to higher-order intuitionistic predicate logic \cite{barendregt1991_pts}.
Fundamentally, the Curry-Howard correspondence associates programs of a type theory with proofs of a logic, and types with formula.
However, the correspondence is not an isomorphism because the logical view does not possess a unique assignment of proofs.
The type theory contains potentially \textit{more} information than the proof derivation.

Cedille is a programming language with a core type theory based on CC \cite{stump2017_cdle,stump2021_cedillecore}.
However, Cedille took an alternative road to obtaining inductive data than what was done in the 1980s.
Instead, CC was modified to add the implicit products of Miquel \cite{miquel2001}, the dependent intersections of Kopylov \cite{kopylov2003}, and an equality type over untyped terms.
The initial goal of Cedille was to find an efficient way to encode inductive data.
This was achieved in 2018 with Mendler-style lambda encodings \cite{firsov2018_mendler}. 
However, the design of Cedille sacrificed certain properties such as the decidability of type checking.
Decidability of type checking was stressed by Kreisel to Scott as necessary to reduce proof checking to type checking because a proof does not, under Kreisel's philosophy, diverge \cite{scott1970}.
This puts into contention if Cedille corresponds to a logic at all.
What remains is to describe the redesign of Cedille such that it does have decidability of type checking and to argue why this state of affairs is preferable.
However, completing this journey requires a deeper introduction into the type theories of the $\lambda$-cube.

\section{System \texorpdfstring{F$^\omega$}{F Omega}}

The following description of System F$^\omega$ differs from the standard presentation in a few important ways:
\begin{enumerate}
    \item the syntax introduced is of a generic form which makes certain definitions more economical,
    \item a bidirectional PTS style is used but weakening is replaced with a well-formed context relation.
\end{enumerate}
These changes do not affect the set of proofs or formula that are derivable internally in the system.

\input{figures/01/syntax.tex}

Syntax consists of three forms: variables ($x, y, z, \ldots$), binders ($\mathfrak{b}$), and constructors ($\mathfrak{c}$).
Every binder and constructor has an associated discriminate or tag to determine the specific syntactic form.
Constructor tags have an associated arity ($\mathfrak{a}$) which determines the number of arguments, or subterms, the specific constructor contains.
A particular syntactic expression will be interchangeably called a syntactic form, a term, or a subterm if it exists inside another term in context.
See Figure~\ref{fig:syntax_f} for the complete syntax of F$^\omega$.
Note that the grammar for the syntax is defined using a BNF-style \cite{floyd1961_bnf} where $t ::= f(t_1, t_2, \ldots)$ represents a recursive definition defining a category of syntax, $t$, by its allowed subterms.
For convenience a shorthand form is defined for each tag to maintain a more familiar appearance with standard syntactic definitions.
Thus, instead of writing $\mathfrak{b}(\lambda, (x : A), t)$ the more common form is used: $\abs{\lambda}{x}{A}{t}$.
Whenever the tag for a particular syntactic form is known the shorthand will always be used instead.

\input{figures/01/ops.tex}

Free variables of syntax is defined by a straightforward recursion that collects variables that are not bound in a set.
Likewise, substitution is recursively defined by searching through subterms and replacing the associated free variable with the desired term.
See Figure~\ref{fig:ops_f} for the definitions of substitution and computing free variables.
However, there are issues with variable renaming that must be solved.
A syntactic form is renamed by consistently replacing bound and free variables such that there is no variable capture.
For example, the syntax $\abs{\lambda}{x}{A}{y\ x}$ cannot be renamed to $\abs{\lambda}{y}{A}{y\ y}$ because it captures the free variable $y$ with the binder $\lambda$.
More critically, variable capture changes the meaning of a term.
There are several rigorous ways to solve variable renaming including (non-exhaustively): De Bruijn indices (or levels) \cite{debruijn1972}, locally-nameless representations \cite{chargueraud2012}, nominal sets \cite{pitts2013_nominal}, locally-nameless sets \cite{pitts2023_lns}, etc.
All techniques incorporate some method of representing syntax uniquely with respect to renaming.
For this work the variable bureaucracy will be dispensed with.
It will be assumed that renaming is implicitly applied whenever necessary to maintain the meaning of a term.
For example, $\abs{\lambda}{x}{A}{y\ x} = \abs{\lambda}{z}{A}{y\ z}$ and the substitution $[x := t]\abs{\lambda}{x}{A}{y\ x}$ unfolds to $\abs{\lambda}{x}{[x := t]A}{[z := t](y\ x)}$.

\input{figures/01/reduction.tex}

The syntax of F$^\omega$ has a well understood notion of reduction (or dynamics, or computation) defined in Figure~\ref{fig:reduction_f}.
This is an \textit{inductive} definition of a two-argument relation on terms.
A given rule of the definition is represented by a collection of premises ($P_1, \ldots, P_n$) written above the horizontal line and a conclusion ($C$) written below the line.
An optional name for the rule (\textsc{Example}) appears to the right of the horizontal line.
An inductive definition induces a structural induction principle allowing reasoning by cases on the rules and applying the induction hypothesis on the premises.
During inductive proofs it is convenient to name the derivation of a premise ($\D{1}, \ldots, \D{n}$).
Moreover, to minimize clutter during proofs the name of the rule is removed.

$$\ExampleRule \quad\quad \ExampleRule[*]$$

Inductive definitions build a finite tree of rule applications concluding with axioms (or leafs).
Axioms are written without premises and optionally include the horizontal line.
The reduction relation for F$^\omega$ consists of three rules and one axiom.
Relations defined in this manner are always the \textit{least} relation that satisfies the definition.
In other words, any related terms must have a corresponding inductive tree witnessing the relation.

The reduction relation (or step relation) models function application anywhere in a term via its axiom, called the $\beta$-rule.
This relation is antisymmetric.
There is a \textit{source} term $s$ and a \textit{target} term $t$, $s \betared t$, where $t$ is the result of one function evaluation in $s$.
Alternatively, $s \betared t$ is read as $s$ \textit{steps} to $t$.
Note that if there is no $\lambda$-term applied to an argument (i.e. no function ready to be evaluated) for a given term $t$ then that term cannot be the source term in the reduction relation.
A term that cannot be a source is called a \textit{value}.
If there exists some sequence of terms related by reduction that end with a value, then all source terms in the sequence are \textit{normalizing}.
If \textit{all} possible sequences of related terms end with a value for a particular source term $s$, then $s$ is \textit{strongly normalizing}.
Restricting the set of terms to a normalizing subset is critical to achieve decidability of the reduction relation.

\input{figures/01/multi.tex}

For any relation $-R-$, the reflexive-transitive closure ($-R^*-$) is inductively defined with two rules as shown in Figure~\ref{fig:multi}.
In the case of the step relation the reflexive-transitive closure, $s \betastar t$, is called the \textit{multistep relation}.
Additionally, when $s \betastar t$ then $s$ \textit{multisteps} to $t$.
It is easy to show that any reflexive-transitive closure is itself transitive.

\begin{lemma}
    Let $R$ be a relation on a set $A$ and let $a, b, c \in A$. If $a\ R^*\ b$ and $b\ R^*\ c$ then $a\ R^*\ c$
\end{lemma}
\begin{proof}
    By induction on $a\ R^*\ b$.

    $\text{Case: }\begin{array}{c} \MultiRefl[*] \end{array}$
    \begin{proofcase}
        It must be the case the $a = b$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \MultiStep[*] \end{array}$
    \begin{proofcase}
        Let $z = t^\prime$, then we have $a\ R\ z$ and $z\ R^*\ b$.
        By the inductive hypothesis (IH) we have $z\ R^*\ c$ and by the transitive rule we have $a\ R^*\ c$ as desired.
    \end{proofcase}
\end{proof}

Two terms are \textit{convertible}, written $t_1 \equiv t_2$, if $\exists\ t^\prime$ such that $t_1 \betastar t^\prime$ and $t_2 \betastar t^\prime$.
Note that this is not the only way to define convertibility in a type theory, but it is the standard method for a PTS.
Convertibility is used in the typing rules to allow syntax forms to have continued valid types as terms reduce.
It may be tempting to view conversion as the reflexive-symmetric-transitive closure of the step relation, but transitivity is not an obvious property.
In fact, proving transitivity of conversion is often a significant effort, beginning with the confluence lemma.

\begin{lemma}[Confluence]
    If $s \betastar t_1$ and $s \betastar t_2$ then $\exists\ t^\prime$ such that $t_1 \betastar t^\prime$ and $t_2 \betastar t^\prime$
\end{lemma}
\begin{proof}
    See Appendix~\ref{ap:a} for a proof of confluence involving a larger reduction relation. Note that F$^\omega$'s step relation is a subset of this relation and thus is confluent.
\end{proof}

\begin{theorem}[Transitivity of Conversion]
    If $a \equiv b$ and $b \equiv c$ then $a \equiv c$
    \label{thm:1:trans}
\end{theorem}
\begin{proof}
    By premises we know $\exists u, v$ such that $a \betastar u$, $b \betastar u$, $b \betastar v$, and $c \betastar v$.
    By confluence, $\exists z$ such that $u \betastar z$ and $v \betastar z$.
    By transitivity of multistep reduction, $a \betastar z$ and $c \betastar z$.
    Therefore, $a \equiv c$.
\end{proof}

\input{figures/01/infer.tex}

Figure~\ref{fig:typing_f} defines the typing relation on terms for F$^\omega$.
As previously mentioned this formulation is different from standard presentations.
Four relations are defined mutually:
\begin{enumerate}
    \item $\Gamma \vdash t \infr T$, to be read as $T$ is the inferred type of the term $t$ in the context $\Gamma$ or, $t$ infers $T$ in $\Gamma$;
    \item $\Gamma \vdash t \cinfr T$, to be read as $T$ is the inferred type, possibly after some reduction, of the term $t$ in the context $\Gamma$ or, $t$ reduction-infers $T$ in $\Gamma$;
    \item $\Gamma \vdash t \chck T$, to be read as $T$ is checked against the inferred type of the term $t$ in the context $\Gamma$ or, $t$ checks against $T$ in $\Gamma$;
    \item $\vdash \Gamma$, to be read as the context $\Gamma$ is well-formed, and thus consists only of types that themselves have a type
\end{enumerate}
Note that there are two \textsc{Pi} rules that restrict the domain and codomain pairs of function types to three possibilities: $(\kind, \kind)$, $(\star, \star)$, and $(\kind, \star)$.
This is exactly what is required by the $\lambda$-cube for this definition to be F$^\omega$.
For the unfamiliar reading these rules is arcane, thus exposition explaining a small selected set is provided.

$\AxiomRuleF$ The axiom rule has one premise, requiring that the context is well-formed.
It concludes that the constant term $\star$ has type $\kind$.
Intuitively, the term $\star$ should be viewed as a universe of types, or a type of types, often referred to as a \textit{kind}.
Likewise, the term $\kind$ should be viewed as a universe of kinds, or a kind of kinds.
An alternative idea would be to change the conclusion to $\Gamma \vdash \star \infr \star$.
This is called the \textit{type-in-type} rule, and it causes the type theory to be inconsistent \cite{girard1972,hurkens1995}.
Note that there is no way to determine a type for $\kind$.
It plays the role of a type only.

$\VarRuleF$ The variable rule is a context lookup.
It scans the context to determine if the variable is anywhere in context and then the associated type is what that variable infers.
This rule is what requires the typing relation to mention a context.
Whenever a type is inferred or checked it is always desired that the context is well-formed.
That is why the variable rule also requires the context to be well-formed as a premise, because it is a leaf relative to the inference relation.
Without this additional premise there could be typed terms in ill-formed contexts.

$\AppRuleF$ The application rule infers the type of the term $f$ and reduces that type until it looks like a function-type.
Once a function type is required it is clear that the type of the term $a$ must match the function-type's argument-type.
Thus, $a$ is checked against the type $A$.
Finally, the inferred result of the application is the codomain of the function-type $B$ with the term $a$ substituted for any free occurrences of $x$ in $B$.
This substitution is necessary because this application could be a type application to a type function.
For example, let $f = \abs{\lambda}{X}{\star}{\text{id}\ X}$ where id is the identity term.
The inferred type of $f$ is then $(X : \star) \to X \to X$.
Let $a = \mathbb{N}$ (any type constant), then $f\ \mathbb{N} \infr [X := \mathbb{N}](X \to X)$ or $f\ \mathbb{N} \infr \mathbb{N} \to \mathbb{N}$.

While this presentation of F$^\omega$ is not standard Lennon-Bertrand demonstrated that it is equivalent to the standard formulation \cite{lennon2021}.
In fact, Lennon-Bertrand showed that a similar formulation is logically equivalent for the stronger CIC.
Thus, standard metatheoretical results such as preservation and strong normalization still hold.

\begin{lemma}[Preservation of F$^\omega$]
    If $\Gamma \vdash s \chck T$ and $s \betastar t$ then $\Gamma \vdash t \chck T$
\end{lemma}
\begin{proof}
    See Appendix~\ref{ap:b} for a proof of preservation of a conservative extension of F$^\omega$, and thus a proof of preservation for F$^\omega$ itself.
\end{proof}

\begin{theorem}[Strong Normalization of F$^\omega$]
    If $\Gamma \vdash t \infr T$ then $t$ and $T$ are strongly normalizing
\end{theorem}
\begin{proof}
    System F$^\omega$ is a subsystem of CC which has several proofs of strong normalization.
    See (non-exhaustively) proofs using saturated sets \cite{geuvers1994_sn_satset}, model theory \cite{terlouw1995_sn}, realizability \cite{ong1993}, etc.
\end{proof}

With strong normalization the convertibility relation is decidable, and moreover, type checking is decidable.
Let \textit{red} be a function that reduces its input until it is either $\star$, $\kind$, a binder, or in normal form.
Note that this function is defined easily by applying the outermost reduction and matching on the resulting term.
Let \textit{conv} test the convertibility of two terms.
Note that this function may be defined by reducing both terms to normal forms and comparing them for syntactic identity.
Both functions are well-defined because F$^\omega$ is strongly normalizing.
Then the functions \textit{infer}, \textit{check}, and \textit{wf} can be mutually defined by following the typing rules.
Thus, type inference and type checking is decidable for F$^\omega$.

While it is true that F$^\omega$ only has function types as primitives several other data types are internally derivable using function types.
For example, the type of natural numbers is defined:
$$\mathbb{N} = (X : \star) \to X \to (X \to X) \to X$$
Likewise, pairs and sum types are defined:
$$A \times B = (X : \star) \to (A \to B \to X) \to X$$
$$A + B = (X : \star) \to ((A \to X) \times (B \to X)) \to X$$
The logical constants true and false are defined:
$$\top = (X : \star) \to X \to X$$
$$\bot = (X : \star) \to X$$
Negation is defined as implying false:
$$\neg A = A \to \bot$$
These definitions are called \textit{Church encodings} and originate from Church's initial encodings of data in the $\lambda$-calculus \cite{church1932,church1933}.
Note that if there existed a term such that $\vdash t \chck \bot$ then trivially for \textit{any} type $T$ we have $\vdash t\ T \chck T$.
Thus, $\bot$ is both the constant false and the proposition representing the principle of explosion from logic.
Moreover, this allows a concise statement of the consistency of F$^\omega$.

\begin{theorem}[Consistency of System F$^\omega$]
    There is no term $t$ such that $\vdash t \chck \bot$
\end{theorem}
\begin{proof}
    Suppose $\vdash t \chck \bot$.
    Let $n$ be the value of $t$ after it is normalized.
    By preservation $\vdash n \chck \bot$.
    Deconstructing the checking judgment we know that $\vdash n \infr T$ and $T \equiv \bot$, but $\bot$ is a value and values like $n$ infer types that are also values.
    Thus, $T = \bot$ and we know that $\vdash n \infr \bot$.
    By inversion on the typing rules $n = \abs{\lambda}{X}{\star}{b}$, and we have $X : \star \vdash b \infr X$.
    The term $b$ can only be $\star$, $\kind$, or $X$, but none of these options infer type $X$.
    Therefore, there does not exist a term $b$, nor a term $n$, nor a term $t$.
\end{proof}

Recall that induction principles cannot be derived internally for any encoding of data \cite{geuvers2001_noind}.
This is not only cumbersome but unsatisfactory as the natural numbers are in their essence the least set satisfying induction.
Ultimately, the issue is that these encodings are too general.
They admit theoretical elements that F$^\omega$ is not flexible enough to express nor strong enough to exclude.

\section{Calculus of Constructions and Cedille}

As previously mentioned, CC is one extension away from F$^\omega$ on the $\lambda$-cube.
Indeed, the two rules \textsc{Pi1} and \textsc{Pi2} can be merged to form CC:
$$\PiRuleCC$$
where now both $K_1$ and $K_2$ are metavariables representing either $\star$ or $\kind$.
Note that no other rules, syntax, or reductions need to be changed.
Replacing \textsc{Pi1} and \textsc{Pi2} with this new \textsc{Pi} rule is enough to obtain a complete and faithful definition of CC.

With this merger types are allowed to depend on terms.
From a logical point of view, this is a quantification over terms in formula.
Hence, why CC is a predicate logic instead of a propositional one according to the Curry-Howard correspondence.
Yet, there is a question about what exactly quantification over terms means.
Surely it does not mean quantification over syntactic forms.

It means, at minimum, quantification over well-typed terms, but from a logical perspective these terms correspond to proofs.
In first order predicate logic the domain of quantification ranges over a set of \textit{individuals}.
The set of individuals represents any potential set of interest with specific individuals identified through predicates expressing their properties.
With proofs the situation is different.
A proof has meaning relative to its formula, but this meaning may not be relevant as an individual in predicate logic.
For example, the proof $2$ for a Church encoded natural number is intuitively data, but a proof that $2$ is even is intuitively not.
In CC, both are merely proofs that can be quantified over.

Cedille alters the domain of quantification from proofs to (untyped) $\lambda$-caluclus terms.
Thus, for Cedille, the proof $2$ becomes the encoding of $2$ and the proof that $2$ is even can \textit{also} be the encoding of $2$.
This is achieved through a notion of \textit{erasure} which removes type information and auxiliary syntactic forms from a term.
Additionally, convertibility is modified to be convertibility of $\lambda$-calculus terms.
However, erasure as it is defined in Cedille enables diverging terms in inconsistent contexts.
The result by Abel and Coquand, which applies to a wide range of type theories including Cedille, is one way to construct a diverging term \cite{abel2020_normalization}.

If terms are able to diverge, in what sense are they a proof?
What a proof is or is not is difficult to say.
As early as Aristotle there are documented forms of argument, Aristotle's syllogisms \cite{aristotle}.
More than a millennium later Euclid's \textit{Elements} is the most well-known example of a mathematical text containing what a modern audience would call proofs.
Moreover, visual renditions of \textit{Elements}, initiated by Byrne, challenge the notion of a proof being an algebraic object \cite{byrne}.
However, the study of proof as a mathematical object dates first to Frege \cite{frege1879} followed soon after by Peano's formalism of arithmetic \cite{peano1889} and Whitehead and Russell's \textit{Principia Mathematica} \cite{whitehead}.
For the kinds of logics discussed by the Curry-Howard correspondence, structural proof theories, the originator is Gentzen \cite{gentzen1935_i,gentzen1935_ii}.
Gentzen's natural deduction describes proofs as finite trees labelled by rules.
Note that this is, of course, a very brief history of mathematical proof.

All of these formulations may be justified as acceptable notions of proof, but the purpose of proof from an epistemological perspective is to provide justification.
It is unsatisfactory to have a claimed proof and be unable to check that it is constructed only by the rules of the proof theory.
This is the situation with Cedille, although rare, there are terms where reduction diverges making it impossible to check a type.
However, it is unfair to levy this criticism against Cedille alone, as well-known type theories also lack decidability of type checking.
For example, Nuprl with its equality reflection rule \cite{allen2000}, and the proof assistant Lean with its notion of casts \cite{moura2021}.
Moreover, Lean has been incredibly successful in formalizing research mathematics including the Liquid Tensor Experiment \cite{liquid_tensor_experiment} and Tao's formalization of The Polynomial Freiman-Ruzsa Conjecture \cite{tao2024_pfr}.
Indeed, not having decidability of type checking does to necessarily prevent a tool from producing convincing arguments.

Ultimately, the definition of proof is a philosophical one with no absolute answer, but this work will follow Gentzen and Kreisel in requiring that a proof is a finite tree, labelled by rules, supporting decidable proof checking.
The reader need only asks themselves which proof they would prefer if the option was available: one that potentially diverges, or one that definitely does not.
If it is the latter, then striving for decidable type theories that are capable enough to reproduce the results obtained by proof assistants like Lean is a worthy goal.

\section{Equality}

A type theory is \textit{intensional} if its propositional equality is able to observe the operational definition of a function.
Within mathematics, functions are defined as a functional relation between their inputs and outputs, but this definition lacks any computational content.
In contrast, dependent type theories are constructive in nature, a function is not merely a functional relation but a stronger notion of an operational term encoding the procedure to produce outputs from inputs.
This additional structure prevents equality of functions from being defined pointwise in situations where the extra structure is observable.
The below variant of Martin-L\"{o}f's intensional identity type is a canonical example of an intensional propositional equality.

% \[
%     \arraycolsep=12pt
%     \def\arraystretch{3}
%     \begin{array}{c}
%         \infer{\Gamma \vdash refl(a) \synth Id_A(a, a)}{
%             \Gamma \vdash a \synth A
%         }
%         \\
%         \infer{\Gamma \vdash elim(p, \absu{\lambda}{x}{b}) \synth P(a, a', p)}{
%             \let\scriptstyle\textstyle
%             \substack{
%                 \Gamma, \ann{x}{A}, \ann{y}{A}, \ann{z}{Id_A(x, y)} \vdash P(x, y, z) \synth \star
%                 \\
%                 \Gamma \vdash p \synth Id_A(a, a')
%                 \quad
%                 \Gamma, \ann{x}{A} \vdash b \synth P(x, x, refl(x))
%             }
%         }
%         \vspace{-.25in}
%         \\
%         elim(refl(a), \absu{\lambda}{x}{b}) = b[x \mapsto a]
%     \end{array}
% \]

Note that the variation of the intensional identity type does matter.
In dependent type theories with an impredicative universe of types Coquand et al. showed that an identity type with a proof-irrelevant cast refutes strong normalization \cite{abel2020_normalization}.
This is an instance of propositional equality being sensitive to other features of a type theory.

\textbf{Function extensionality}, the formal statement of which is shown below, is a reasoning principle that is often lost in the intensional setting.
$$\abs{\Pi}{f, g}{X \to Y}{
    (\abs{\Pi}{x}{X}{Id_Y(f\ x, g\ x)}) \to Id_{X \to Y}(f, g)
}$$
Postulating function extensionality as an axiom breaks canonicity but as a reasoning principle, particularly when formalizing mathematics, it is significantly more convenient.
Absolving the tension between a dependent type theory with good metatheoretical properties, which intensional propositional equality gives, and a theory that admits function extensionality has and is a major goal of current research.
The additional constructive information supplied to define a function in dependent type theory gets in the way of strong reasoning principles.
Thus, one might say that equality is about what properties of an object one wishes to \textit{ignore} as opposed to some philosophically ordained notion.
However, the intensional information of a function is also potentially important.
For computer science, the intensional behavior may be critical to security properties or computational complexity (e.g. it may be undesirable to equate merge sort with bubble sort).

Outside of dependent type theory a common definition of equality is Leibniz equality, which equates two objects only if any predicate that holds of one object necessarily holds of the other.
Leibniz equality can be viewed as a Church-encoded version of the identity type which means that these different definitions of propositional equality imply one another.
However, they are only isomorphic if quantification is parametric and function extensionality holds \cite{abel2020_equality}.

Attempting to bridge the gap between intensional and extensional features Streicher proposed his Axiom K in 1993 \cite{streicher1993}.
Today this axiom is better known as Uniqueness of Identity Proofs (UIP), specified below, but which intuitively means that all proofs of an equality are themselves equal.
$$\abs{\Pi}{x, y}{X}{
    \abs{\Pi}{p, q}{Id_X(x, y)}{
        Id_{Id_X(x, y)}(p, q)
    }
}$$
For many years it was an open question whether or not the intensional identity type always satisfied UIP as it was generally believed there is only one equality proof, the reflexivity proof.

In 1995 Hofmann answered this equation negatively: intensional identity types need not always satisfy UIP, but he did not stop there.
Indeed, Hofmann investigated how many extensional notions (function extensionality, quotient types, subset types, UIP, and propositional extensionality) might be added to dependent type theory.
To accomplish this goal he modelled identity types in two separate ways.
First, as equivalence relations defined inductively on type structure, a precursor to the setoid model of type theory.
Second, as groupoids, a precursor to the univalent model of type theory.
Unfortunately, his models did not achieve all of his goals where certain metatheoretical properties and desirable definitional equalities are lost depending on the interpretation \cite{hofmann1995, hofmann1996}.
Regardless, his contributions broke through a long-standing assumption and opened a world of varying interpretations of intensional propositional equality.

\textbf{Propositional extensionality} means that equivalence of propositions is equivalent to equality of propositions.
Stated formally below, equivalence in this context means mutual implication.
Propositions are assumed to be distinct from types (i.e. propositions are usually assumed to satisfy some variation of proof-irrelevance).
The universe \textsc{Prop} captures this distinction between the more general universe of types and those interpreted as propositions.
$$\abs{\forall}{P, Q}{\textsc{Prop}}{
    (P \leftrightarrow Q) \leftrightarrow (P = Q)
}$$

\textbf{Quotient types} build a type $A/\!\!\sim$ from a carrier type $A$ and an equivalence relation on that carrier type $\sim$ such that the propositional equality for elements of $A/\!\!\sim$ respects the equivalence relation.
Mathematical practice utilizes quotients extensively to construct objects from some carrier set and equations on the elements of that set.
For example, the simplest mathematical quotient is the set of rational numbers which is the quotient of fractions (i.e. pairs of integers) and the equivalence relation $\abs{\forall}{(n_1, d_1), (n_2, d_2)}{\mathbb{Z \times Z}}{n_1d_2 = n_2d_1}$.
Many algebraic objects are also quotients, such as groups, fields, modules, and tensor products of modules to name a few.

The equivalence relation for a quotient type (or quotient set) induces a partition of the elements of that type into equivalence classes.
If a canonical representative of an equivalence class can be effectively computed then quotient types are definable in most dependent type theories.
It is the other variant, where a canonical representative can not be computed, that is problematic to encode.
The real numbers and multi-sets of elements that are not orderable are two examples of quotient types where a canonical representative is uncomputable \cite{li2015}.
Indeed, the set-theoretic axiom of choice is equivalent to the ability to pick a unique representative from equivalence classes for any arbitrary equivalence relation \cite{lof2009}.
Additionally, if a type theory supports an impredicative universe of types, then adding quotients such as the real numbers causes inconsistency \cite{chicli2002}.

Quotients is an important feature of dependent type theories.
Indeed, some theories are designed, at least partially, to explicitly support quotients.
The Lean proof assistant in particular incorporates quotients axiomatically in its standard library, a choice that is arguably responsible for mathematicians interest in it as a tool for formalizing theorems (if only because it more closely matches a culture that mathematicians expect) \cite{carneiro2019}.
Quotient inductive types is one technique for adding quotients to type theory which extends inductive definitions with equational variants that the constructors must satisfy.
With quotient inductive types the integers can be defined succinctly as the following inductive type.
\begin{align*}
    \mathbb{Z} :=\ &0 : \mathbb{Z} \\
        &succ : \mathbb{Z} \to \mathbb{Z} \\
        &pred : \mathbb{Z} \to \mathbb{Z} \\
        &succ\_pred : \abs{\forall}{i}{\mathbb{Z}}{succ\ (pred\ i) = i} \\
        &pred\_succ : \abs{\forall}{i}{\mathbb{Z}}{pred\ (succ\ i) = i}
\end{align*}
Quotient inductive definitions were given a formal foundation in 2007 and demonstrated to be useful for direct internalization of dependent type theories \cite{altenkirch2016_quotients, dijkstra2017}.

\subsection{Extensional Type Theories}

Extensional type theory does not have any of the aforementioned extensionality problems, as its namesake might suggest.
The distinguishing feature that transforms an intensional theory into an extensional one is the equality reflection rule, listed below.
It is difficult to pin an exact year when equality reflection is first introduced but, some of Martin-L\"{o}f's early systems are extensional type theories and likewise some of the earliest proof assistant implementations were extensional type theories (e.g. Nuprl).
% \[
%     \arraycolsep=12pt
%     \def\arraystretch{3}
%     \begin{array}{c}
%         \infer{\Gamma \vdash a = a' }{
%             \Gamma \vdash p \synth Id_A(a, a')
%         }
%     \end{array}
% \]
Equality reflection collapses definitional and propositional equality.
If propositional equality is undecidable then definitional equality becomes undecidable as a consequence.
Thus, even though a proof assistant such as Nuprl supports quotient types, subset types, and other extensionality properties it lacks decidability of conversion checking.
To counteract these restrictions users of Nuprl work with derivation trees as opposed to proof terms \cite{constable1986}.

This trade-off, between extensional properties and good metatheoretical properties has lead the design of dependent type theories away from full-blown equality reflection.
Instead, designers have attempted to leverage other techniques, interpreting types as setoids or groupoids, or adding ad hoc equality reflection via rewriting.
However, in 2016 Andrej Bauer demonstrated that equality reflection could be designed around effect handlers.
The undecidability of equality reflection is side-stepped as the provided handler resolves the proof obligations generated by definitional equality by what is effectively a try-catch exception handler \cite{bauer2016}.

More recently, in 2019, it was shown that extensional type theory can be modelled in an intensional type theory that satisfies UIP and function extensionality \cite{winterhalter2019}.
This result was later strengthened to modelling extensional type theory in a weak theory \cite{boulier2019}.
A type theory is \textit{weak} if $\beta$ and $\eta$ convertibility rules have to be expressed in the syntax meaning that definitional equality is $\alpha$-equivalence.
These results argue that extensional dependent type theory \textit{is} intensional type theory with UIP and function extensionality.
Especially because equality reflection hampers any procedure to automatically decide definitional equalities anyway, making the reduction to a weak intensional type theory moot.

Bishop, in 1967, showed that much of mathematical analysis could be carried out in a constructive setting.
His constructive analysis heavily leveraged ``Bishop sets'' or \textit{setoids} which incorporate a carrier set of elements together with an equivalence relation about those sets \cite{bishop1967}.
As mentioned previously, setoids were used by Hofmann to construct models of type theory that supported some extensional properties.
Although Hofmann's models did not capture all the desired properties for an intensional type theory with extensional features the idea of modelling types as setoids persisted.

Altenkirch in particular has heavily invested in dependent type theories in this style.
In 1999, Altenkirch improved Hofmann's setoid model to a type theory that admits large eliminations, enjoys function extensionality, has decidable definitional equality, and satisfies canonicity.
Moreover, he stresses that the metatheory used to accomplish this model is itself an \textit{intensional} one, whereas Hofmann's metatheory was extensional.
Altenkirch accomplished this task by incorporating a proof irrelevant universe of propositions which will be a common theme for type theories supporting extensional features.
However, his 1999 attempt did not contain dependent types or quotients though Altenkirch claimed quotients could be easily added \cite{altenkirch1999}.

\textbf{Observational Type Theory (OTT)} was introduced years later (in 2007) by Altenkirch and McBride.
The core ideas remained the same: semantically types are interpreted as setoids and universe of propositions is added to an intensional type theory.
What OTT introduces, however, is defining propositional equality by recursion on type constructors.
This recursive definition grants greater flexibility in how equality is treated for individual type formers.
Elimination of propositional equality is additionally defined to act via coercions which are also defined recursively on type structure.
Critically, propositional equality for function types (and coercions related to it) can be defined to admit pointwise equality directly.
Unfortunately, the definition of OTT makes the connection to the setoid model less clear which prevents an obvious addition of quotient types and a universe of types \cite{altenkirch2007}.

In 2022, Pujet et al. introduce an improvement of OTT and move past many of the limitations of the prior work.
The reduction of definitional equality is altered to reduce terms to weak head normal form as opposed to a full normal form.
Stopping at weak head normal form allows the head constructor to determine the corresponding propositional equality rule.
Thus, the recursive definition of propositional equality that was the namesake of OTT is lifted to standard typing rules.
In the case of function types, this means that pointwise equality can be directly defined as the typing rule for the identity type.
Therefore, function extensionality is baked into the typing judgments and there is no way for the intensional difference of functions to be observed by the identity type.
Like prior developments, the techniques of Pujet et al. hinge on a proof-irrelevant universe of propositions.
However, the authors note that a critical difference from prior attempts at OTT is that propositions satisfy definitional proof-irrelevance (as opposed to propositional proof-irrelevance) which prevents equational proof obligations muddying goals of type judgments.
Moreover, many desirable properties are proven including: propositional extensionality, UIP, strong normalization, consistency, and decidability of type checking.
Quotient types are also added provided the equivalence relation is proof-irrelevant (in other words, provided the equivalence relation is expressed in terms of the universe of propositions) \cite{pujet2022}.

\textbf{Setoid Type Theory (SeTT)}, meanwhile, is the evolution of Altenkirch's work on OTT.
Based on the original root idea by Altenkirch of constructing a setoid model directly in an intensional type theory as he did in 1999, the first introduction of SeTT (in 2019) improved on the first attempt by making function extensionality, propositional extensionality, and the elimination principle for the identity type hold definitionally.
Indeed, the new model is noted to be a \textit{syntactic translation} from a setoid type theory to an intensional type theory.
A critical property of syntactic translations is that definitional equalities are all preserved \cite{altenkirch2019}.
Syntactic translations, in the words of Altenkirch, give a way to bootstrap extensionality principles from intensional type theories.
The syntactic translation is later improved to internalize a universe of setoids (i.e. a universe of types) \cite{altenkirch2021}.

\subsection{Univalent Type Theories}

While setoid type theory was and continues to be explored there is a competing interpretation of types foreshadowed by Hofmann as groupoids.
A \textit{groupoid} is a collection of objects and invertible morphisms between those objects that satisfy identity and associativity laws.
Setoids are a special case of groupoids.
There is a further generalization to $n$-groupoids that allows invertible 2-morphisms with morphisms as objects, invertible 3-morphisms with 2-morphisms as objects, and so on up-to invertible $n$-morphisms with $n-1$-morphisms as objects.
A limiting process can be applied to this generalization obtaining $\omega$-groupoids that have infinite towers of invertible $i$-morphisms with $i-1$-morphisms as objects.
A groupoid and its generalizations are studied in Category Theory, but for the purposes of modeling dependent type theory the core idea is that propositional equality has a rich proof structure including interesting equality proofs between equality proofs.
Of course, as a natural consequence, UIP is refuted in a non-degenerate groupoid model.

In 2006, the mathematician Voevodsky began studying type theory has an alternative foundation for mathematics after expressing doubts about the correctness of results in the mathematical literature.
He proposed the Univalence Axiom, shown below, as a desirable feature of dependent type theory because, in his opinion, it accurately modelled the transport of properties between objects that mathematicians take for granted \cite{voevodsky2006}.
From this point forward, a \textit{univalent} type theory is any type theory that admits univalence either as an axiom or as a derivable notion.
Upon closer inspection of the Univalence Axiom it becomes clear that it is a generalization of propositional extensionality.
Where propositional extensionality states an equivalence between equivalent propositions and equal propositions, the Univalence Axiom states that an isomorphism between types is isomorphic to an equality between types.
$$\abs{\forall}{A, B}{\star}{
    (A \simeq B) \simeq (A = B)
}$$

\textbf{Homotopy Type Theory (HoTT)} is one of the first univalent theories proposed from these observations.
HoTT interprets the intensional identity type as consisting of homotopy equivalences between types and terms.
A \textit{homotopy} between two functions $f, g : X \to Y$ is a continuous map $h : \mathbb{I} \to X \to Y$ such that $h(0) = f$ and $h(1) = g$ and where $\mathbb{I} = [0, 1]$ is the unit interval.
It is a mathematical device to express the smooth deformation of one function to another.
Of course, homotopies are generalizable beyond the case of first-order functions.
However, notice that if the identity type consists of homotopies then the metatheory of HoTT necessarily appeals to the existence of a set of real numbers (i.e. $\mathbb{I} \subseteq \mathbb{R}$).
Nonetheless, HoTT has been used effectively to build a foundation of mathematics and to work synthetically in the field of Homotopy Theory \cite{hottbook}.

The Univalence Axiom was originally just that, an axiom, but this breaks canonicity.
Dependent type theories throughout their conception have kept great interest in maintaining a dual purpose: as a programming language \textit{and} as a logic.
An initial model construction of univalent type theory in the category of simplicial sets was noted as problematic because of its use of a classical metatheory \cite{kapulkin2012}.
Later, Bezem demonstrated that Voevodsky's simplicial sets model is \textit{necessarily} classical \cite{bezem2015}.
For deciding some metatheoretical properties, such as consistency, a classical model is sufficient, but as a road to providing a constructive derivation of the Univalence Axiom these models are not helpful.
Fortunately, Bezem was also able to show that a model with a constructive metatheory is possible using cubical sets which was an important step towards providing a computational basis to the Univalence Axiom \cite{bezem2014}.

\textbf{Cubical type theory} provides a computational interpretation to the Univalence Axiom by lifting the unit interval from the semantic domain to syntax.
The $\lambda$-calculus is extended with two constants $0$ and $1$ and a theory of \textit{names} representing distinguished points inside the interval.
In one such development by Cohen et al. various operations are also assumed to hold on elements of the interval including maximums, minimums, and involutions forming a De Morgan algebra which the authors claim simplify semantic justifications \cite{cohen2016}.
Soon after, more variations of cubical type theories were considered all tweaking the underlying operations allowed on the unit interval.
In 2018, Pitts et al. explored the minimal set of axioms in a topos theoretic setting needed to model different kinds of cubical type theories.
Propositional equality, called the path type in cubical type theory, is constructed as functions with domain $\mathbb{I}$.
Various axioms are postulated including connectedness, distinctness of end-points, a connection algebra on the names of the interval, and a set of ``face-formulas'' \cite{pitts2018}.

However, two years later Cavallo et al. observe that a more minimal axiomatic framework works for achieving the goals of Pitts et al. \cite{cavallo2020}.
With their approach there is no need for the diagonals used by Cohen or the connection algebra used by Pitts.
Indeed, the minimal set of axioms are:
\begin{align*}
    \text{ax}_1 &: \abs{\Pi}{P}{\mathbb{I} \to \star}{
        (\abs{\Pi}{i}{\mathbb{I}}{P\ i + \neg(P\ i)}) \to
        (\abs{\Pi}{i}{\mathbb{I}}{P\ i})
        + (\abs{\Pi}{i}{\mathbb{I}}{\neg(P\ i)})
    } \\
    \text{ax}_2 &: \neg(0 = 1)
\end{align*}
The minimal \textit{face formulas} needed is a universe of propositions satisfying the following three rules:
\begin{align*}
    (-\text{ is }0) &: \mathbb{I} \to \Phi
    &\text{ax}_3 &: \abs{\Pi}{i}{\mathbb{I}}{
        [(i\text{ is 0})] = (i = 0)
    } \\
    (-\text{ is }1) &: \mathbb{I} \to \Phi
    &\text{ax}_4 &: \abs{\Pi}{i}{\mathbb{I}}{[(i\text{ is 1})] = (i = 1)} \\
    \vee &: \Phi \to \Phi \to \Phi
    &\text{ax}_5 &: \abs{\Pi}{\varphi, \psi}{\Phi}{[\varphi \vee \phi] = [\varphi] \vee [\psi]}
\end{align*}
where $[\phi]$ is a function from syntactic propositional formulas to propositions.
With only the minimal set of axioms the model construction only supports weakened rules of propositional equality, but it serves as a consistent basis to explain extensions to different cubical type theory structures.
The work of Cavallo et al. arguably presents the essence of cubical type theory:
\begin{enumerate}
    \item {
        the interval must be connected, which prevents a discretization and maintains an internal continuity for smooth deformations (i.e. homotopy);
    }
    \item {
        the two endpoints must be distinct, which prevents collapsing the interval to a unit type and obliterating the internal structure;
    }
    \item {
        and a description of face formulas that encode a simple universe of propositions that allow distinguishing endpoints and disjunctive combination.
    }
\end{enumerate}
Like setoid type theories, cubical type theories also rely on a description of a universe of propositions to encode statements about propositional equality.
This shared feature hints at the virality of equality.
It is not enough to change the rules of equality, one must control the space that equality can act upon as well.

Univalent type theory has been effective in incorporating extensional features into a dependent type theory without hampering metatheoretical properties.
For instance, quotient types are representable when an additional rule truncating the higher-order equality structure is also included \cite{kraus2020}.
Moreover, a new variant of inductive types named \textit{higher inductive types} generalizes quotient inductive types allowing for the definition of homotopy spaces directly as inductive types \cite{angiuli2017}.
Cubical techniques are also useful in constructing setoid type theories that support definitional UIP by adding the appropriate rules to define a degenerate groupoid model \cite{sterling2020}.
Finally, cubical type theory has recently been implemented as Cubical Agda which extends the development with records, coinductive types, and dependent pattern matching on higher inductive types \cite{vezzosi2021}.

Univalent type theories can have some undesirable side effects from a programmatic perspective.
For instance, Hofmann noticed early on that in a groupoid model of types it can be the case that $\mathbb{N} = \mathbb{Z}$ because propositional equality is isomorphism.
Altenkirch notes that any construction in a univalent type theory is necessarily stable under homotopy equivalence \cite{altenkirch2016}.
While a desirable situation for mathematics, programming languages often want even bit-identical representations to not be considered equal because the types capture external notions.
Indeed, Voevodsky's early proposals for extension to dependent type theory contain two separate propositional equalities, one to capture isomorphism and support univalence, and one to keep the strict equalities of the original identity type \cite{voevodsky2013}.
Today, Voevodsky's proposed type theory would be called a \textit{two-level} type theory (2LTT).

The core idea behind a two-level type theory is to have two universes of types, an ``inner'' univalent universe satisfying the univalence axiom, and an ``outer'' strict universe satisfying UIP.
This setup can be viewed as an internalization of the ``inner'' theories metatheory as the ``outer'' theory.
Indeed, some statements in HoTT are metatheoretical in nature and can not be expressed in HoTT alone.
For example, the definition of semi-simplicial types (an object used in Homotopy Theory) in a two-level type theory appeals to a natural number in the ``outer'' theory as a parameter.

There need not be a subsumption requirement between the ``inner'' universes and the ``outer'' universes, only an embedding from the ``inner'' to the ``outer'' is required.
Annenkov et al. note in their formulation of 2LTT that the context of the type theory is shared between both theories though the authors concede that this does not need to be the case.
However, critically the dependent function and dependent pair types agree between both theories allowing for intercommunication in their formulation \cite{annenkov2017}.
Capriotti expands upon the foundational aspects of 2LTT showing a conservativity result with respect to HoTT thus confirming that including the ``outer'' theory does not break any internal results constructed in the ``inner'' theory \cite{capriotti2017}.

Angiuli adapts 2LTT when introducing Cartesian Cubical Type Theory to incorporate a strict equality and justify a computational semantics in the same spirit as the one used for Nuprl \cite{angiuli2019}.
Indeed, 2LTT even allows a notion of strict proposition (i.e. a universe of propositions that are definitionally irrelevant).
However, Gilbert the originator of strict propositions, notes that if a strict equality is included in the universe of strict propositions then univalence is no longer compatible hinting at some limitations \cite{gilbert2019}.

\subsection{Miscellaneous Type Theories}

Not all dependent type theories fit neatly into the above story.
Indeed, Cedille itself does not.
To conclude the tour of type theories this subsection reflects on these theories.

\textbf{Quantitative Type Theory} augments any dependent type theory with usage annotations for the corresponding variables in context.
These usage annotations restrict how often a given variable is used in the term body.
Atkey, improving on prior work by McBride, describes a general intensional quantitative type theory parameterized by a semiring of usage annotations, the most familiar variant being $\{ 0, 1, \omega \}$ or \textit{erased}, \textit{linear}, and \textit{unrestricted} \cite{atkey2018}.
Idris 2 is an implementation of quantitative type theory that demonstrates how session types can be effectively used in a setting with usage annotations \cite{brady2021}.

\textbf{Rewriting} can be added to dependent type theory in a multitude of different ways to achieve ad hoc equality reflection.
Cockx investigates a Rewriting Type Theory in detail in his work where rewriting allows the addition of computational axioms.
He notes that rewriting can be used to postulate quotients and higher inductive types.
In particular, rewriting is safely added to a dependent type theory by restricting the left hand-side to linear patterns, where rewrite rules must be orthogonal (i.e. a term can not be an instance of two or more rewrites on the left-hand side), and each rewrite satisfies a technical triangle property (to guarantee confluence).
With these restrictions the addition of rewrites does not break confluence or consistency while also maintaining a modular framework for adding rewrite rules \cite{cockx2020, cockx2021}

\textbf{Zombie}, a close cousin to Cedille, is a dependently typed programming language allowing arbitrary recursion.
For definitional equality, Zombie does not beta-reduce expressions to normal forms and instead uses congruence closure.
Moreover, because Zombie allows non-terminating values, it uses a call by value reduction strategy to prevent non-terminating proof terms from tricking the type system.
Like Cedille, Zombie includes irrelevant function spaces and a heterogeneous equality type.
Additionally, it refutes function extensionality \cite{sjoberg2017}.

\textbf{Iso-type systems} generalize the notion of an iso-recursive type to any type.
In these systems definitional equality is $\alpha$-equivalence while leveraging cast annotations to recover more general conversion rules.
In this respect, an iso-type system is a \textit{weak} type theory with limited automation capabilities in its definitional equality \cite{yang2019}.

\section{Thesis}

Cedille is a powerful type theory capable of deriving inductive data with relatively modest extension and modification to CC.
However, this capability comes at the cost of decidability of type checking and thus, in the opinion of Kreisel, the cost of a Curry-Howard correspondence to a proof theory.
A redesign of Cedille that focuses on maintaining a proof-theoretic view recovers decidability of type checking while still solving the original goals of Cedille.
Although this redesign does prevent some constructions from being possible, the new balance struck between capability and complexity is desirable because of a well-behaved metatheory.

\section{Contributions}

\textbf{Chapter 2}\quad defines the Cedille2 Core (CC2) theory, including its syntax, and typing rules.
Erasure from Cedille is rephrased as a projection from proofs to objects.
Basic metatheoretical results are proven including: confluence, preservation, and classification.

\textbf{Chapter 3}\quad models CC2 in F$^\omega$ obtaining a strong normalization result for proof normalization.
This model is a straightforward extension of a similar model for CC.
Critically, proof normalization is not powerful enough to show consistency nor object normalization.
Additionally, CC2 is shown to be a conservative extension of F$^\omega$.

\textbf{Chapter 4}\quad models CC2 in CDLE obtaining consistency for CC2.
Although CDLE is not strongly normalizing it still possess a realizability model which justifies its logical consistency.
CC2 is closely related to CDLE which makes this models straightforward to accomplish.
Moreover, a selection of axioms added to CC2 is shown to recover much of CDLEs features.

\textbf{Chapter 5}\quad proves object normalization from proof normalization and consistency.
The $\varphi$, or cast, rule is the only difficulty after proof normalization and consistency.
However, any proof can be translated into a new proof that contains no cast rules.
Applying this observation yields an argument to obtain full object normalization.

\textbf{Chapter 6}\quad with normalization for both proofs and objects a well-founded type checker is defined.
This implementation leverages normalization-by-evaluation and other basic techniques like pattern-based unification.
The tool it benchmarked to demonstrate reasonable performance.

\textbf{Chapter 7}\quad contains derivations of generic inductive data, quotient types, large eliminations, constructor subtyping, and inductive-inductive data.
All of these constructions are possible in Cedille but require modest modifications to derive in Cedille2.

\textbf{Chapter 8}\quad concludes with a collection of open conjectures and questions.
Cedille2 at the conclusion of this work is still in its infancy.