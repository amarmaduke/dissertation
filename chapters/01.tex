\chapter{Introduction}
\input{figures/01/typing_macros.tex}
\input{figures/01/reduction_macros.tex}

Type theory is a tool for reasoning about assertions of some domain of discourse.
When applied to programming languages, that domain is the expressible programs and their properties.
Of course, a type theory may be rich enough to express detailed properties about a program, such that it halts or returns an even number.
Therein lies a tension between what properties a type theory can faithfully (i.e. consistently) encode and the complexity of the type theory itself.
If the theory is too complex then it may be untenable to prove that the type theory is well-behaved.
Indeed, the design space of type theories is vast, likely infinite.
When incorporating features the designer must balance complexity against capability.

Modern type theory arguably began with Martin-L\"{o}f in the 1970s and 1980s when he introduced a dependent type theory with the philosophical aspirations of being an alternative foundation of mathematics \cite{lof1975,lof1984}.
Soon after in 1985, the Calculus of Constructions (CC) was introduced by Coquand \cite{coquand1985,coquand1986}.
Inductive data (e.g. natural numbers, lists, trees) was shown by Guevers to be impossible to derive in CC \cite{geuvers2001_noind}.
Nevertheless, inductive data was added as an extension by Pfenning \cite{pfenning1989} and the Calculus of Inductive Constructions (CIC) became the basis for the proof assistant Rocq \cite{paulin-mohring1993}.

In the early 1990s Barendregt introduced a generalization to Pure Type Systems (PTS) and studied CC under his now famous $\lambda$-cube \cite{barendregt1990_cube,barendregt1991_pts}.
The $\lambda$-cube demonstrated how CC could be deconstructed into four essential sorts of functions.
At its base was the Simply Typed Lambda Calculus (STLC) a type theory introduced in the 1940s by Church to correct logical consistency issues in his (untyped) $\lambda$-calculus \cite{church1940_stlc}.
The STLC has only basic functions found in all programming languages.
System F, a type theory introduced by Girard \cite{girard1972,girard1989} and independently by Reynolds \cite{reynolds1974_systemf}, is obtained from STLC by adding quantification over types (i.e. polymorphic functions).
Adding a copy of STLC at the type-layer, functions from types to types, yields System F$^\omega$.
Finally, the addition of quantification over terms or functions from terms to types, completes CC.
While this is not the only path through the $\lambda$-cube to arrive at CC it is the most well-known and the most immediately relevant.

Perhaps surprisingly, all the systems of the $\lambda$-cube correspond to a logic.
In the 1970s Curry circulated his observations about the STLC corresponding to intuitionistic propositional logic \cite{howard1980}.
Reynolds and Girard's combined work demonstrated that System F corresponds to second-order intuitionistic propositional logic \cite{girard1972,reynolds1974_systemf,reynolds1983}.
Indeed, Barendregt extended the correspondence to all systems in his $\lambda$-cube noting System F$^\omega$ as corresponding to higher-order intuitionistic propositional logic and CC as corresponding to higher-order intuitionistic predicate logic \cite{barendregt1991_pts}.
Fundamentally, the Curry-Howard correspondence associates programs of a type theory with proofs of a logic, and types with formula.
However, the correspondence is not an isomorphism because the logical view does not possess a unique assignment of proofs.
The type theory contains potentially \textit{more} information than the proof derivation.

Cedille is a programming language with a core type theory based on CC \cite{stump2017_cdle,stump2021_cedillecore}.
However, Cedille took an alternative road to obtaining inductive data than what was done in the 1980s.
Instead, CC was modified to add the implicit products of Miquel \cite{miquel2001}, the dependent intersections of Kopylov \cite{kopylov2003}, and an equality type over untyped terms.
The initial goal of Cedille was to find an efficient way to encode inductive data.
This was achieved in 2018 with Mendler-style lambda encodings \cite{firsov2018_mendler}. 
However, the design of Cedille sacrificed certain properties such as the decidability of type checking.
Decidability of type checking was stressed by Kreisel to Scott as necessary to reduce proof checking to type checking because a proof does not, under Kreisel's philosophy, diverge \cite{scott1970}.
This puts into contention if Cedille corresponds to a logic at all.
What remains is to describe the redesign of Cedille such that it does have decidability of type checking and to argue why this state of affairs is preferable.
However, completing this journey requires a deeper introduction into the type theories of the $\lambda$-cube.

\section{System \texorpdfstring{F$^\omega$}{F Omega}}

The following description of System F$^\omega$ differs from the standard presentation in a few important ways:
\begin{enumerate}
    \item the syntax introduced is of a generic form which makes certain definitions more economical,
    \item a bidirectional PTS style is used but weakening is replaced with a well-formed context relation.
\end{enumerate}
These changes do not affect the set of proofs or formula that are derivable internally in the system.

\input{figures/01/syntax.tex}

Syntax consists of three forms: variables ($x, y, z, \ldots$), binders ($\mathfrak{b}$), and constructors ($\mathfrak{c}$).
Every binder and constructor has an associated discriminate or tag to determine the specific syntactic form.
Constructor tags have an associated arity ($\mathfrak{a}$) which determines the number of arguments, or subterms, the specific constructor contains.
A particular syntactic expression will be interchangeably called a syntactic form, a term, or a subterm if it exists inside another term in context.
See Figure~\ref{fig:syntax_f} for the complete syntax of F$^\omega$.
Note that the grammar for the syntax is defined using a BNF-style \cite{floyd1961_bnf} where $t ::= f(t_1, t_2, \ldots)$ represents a recursive definition defining a category of syntax, $t$, by its allowed subterms.
For convenience a shorthand form is defined for each tag to maintain a more familiar appearance with standard syntactic definitions.
Thus, instead of writing $\mathfrak{b}(\lambda, (x : A), t)$ the more common form is used: $\abs{\lambda}{x}{A}{t}$.
Whenever the tag for a particular syntactic form is known the shorthand will always be used instead.

\input{figures/01/ops.tex}

Free variables of syntax is defined by a straightforward recursion that collects variables that are not bound in a set.
Likewise, substitution is recursively defined by searching through subterms and replacing the associated free variable with the desired term.
See Figure~\ref{fig:ops_f} for the definitions of substitution and computing free variables.
However, there are issues with variable renaming that must be solved.
A syntactic form is renamed by consistently replacing bound and free variables such that there is no variable capture.
For example, the syntax $\abs{\lambda}{x}{A}{y\ x}$ cannot be renamed to $\abs{\lambda}{y}{A}{y\ y}$ because it captures the free variable $y$ with the binder $\lambda$.
More critically, variable capture changes the meaning of a term.
There are several rigorous ways to solve variable renaming including (non-exhaustively): De Bruijn indices (or levels) \cite{debruijn1972}, locally-nameless representations \cite{chargueraud2012}, nominal sets \cite{pitts2013_nominal}, locally-nameless sets \cite{pitts2023_lns}, etc.
All techniques incorporate some method of representing syntax uniquely with respect to renaming.
For this work the variable bureaucracy will be dispensed with.
It will be assumed that renaming is implicitly applied whenever necessary to maintain the meaning of a term.
For example, $\abs{\lambda}{x}{A}{y\ x} = \abs{\lambda}{z}{A}{y\ z}$ and the substitution $[x := t]\abs{\lambda}{x}{A}{y\ x}$ unfolds to $\abs{\lambda}{x}{[x := t]A}{[z := t](y\ x)}$.

\input{figures/01/reduction.tex}

The syntax of F$^\omega$ has a well understood notion of reduction (or dynamics, or computation) defined in Figure~\ref{fig:reduction_f}.
This is an \textit{inductive} definition of a two-argument relation on terms.
A given rule of the definition is represented by a collection of premises ($P_1, \ldots, P_n$) written above the horizontal line and a conclusion ($C$) written below the line.
An optional name for the rule (\textsc{Example}) appears to the right of the horizontal line.
An inductive definition induces a structural induction principle allowing reasoning by cases on the rules and applying the induction hypothesis on the premises.
During inductive proofs it is convenient to name the derivation of a premise ($\D{1}, \ldots, \D{n}$).
Moreover, to minimize clutter during proofs the name of the rule is removed.

$$\ExampleRule \quad\quad \ExampleRule[*]$$

Inductive definitions build a finite tree of rule applications concluding with axioms (or leafs).
Axioms are written without premises and optionally include the horizontal line.
The reduction relation for F$^\omega$ consists of three rules and one axiom.
Relations defined in this manner are always the \textit{least} relation that satisfies the definition.
In other words, any related terms must have a corresponding inductive tree witnessing the relation.

The reduction relation (or step relation) models function application anywhere in a term via its axiom, called the $\beta$-rule.
This relation is antisymmetric.
There is a \textit{source} term $s$ and a \textit{target} term $t$, $s \betared t$, where $t$ is the result of one function evaluation in $s$.
Alternatively, $s \betared t$ is read as $s$ \textit{steps} to $t$.
Note that if there is no $\lambda$-term applied to an argument (i.e. no function ready to be evaluated) for a given term $t$ then that term cannot be the source term in the reduction relation.
A term that cannot be a source is called a \textit{value}.
If there exists some sequence of terms related by reduction that end with a value, then all source terms in the sequence are \textit{normalizing}.
If \textit{all} possible sequences of related terms end with a value for a particular source term $s$, then $s$ is \textit{strongly normalizing}.
Restricting the set of terms to a normalizing subset is critical to achieve decidability of the reduction relation.

\input{figures/01/multi.tex}

For any relation $-R-$, the reflexive-transitive closure ($-R^*-$) is inductively defined with two rules as shown in Figure~\ref{fig:multi}.
In the case of the step relation the reflexive-transitive closure, $s \betastar t$, is called the \textit{multistep relation}.
Additionally, when $s \betastar t$ then $s$ \textit{multisteps} to $t$.
It is easy to show that any reflexive-transitive closure is itself transitive.

\begin{lemma}
    Let $R$ be a relation on a set $A$ and let $a, b, c \in A$. If $a\ R^*\ b$ and $b\ R^*\ c$ then $a\ R^*\ c$
\end{lemma}
\begin{proof}
    By induction on $a\ R^*\ b$.

    $\text{Case: }\begin{array}{c} \MultiRefl[*] \end{array}$
    \begin{proofcase}
        It must be the case the $a = b$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \MultiStep[*] \end{array}$
    \begin{proofcase}
        Let $z = t^\prime$, then we have $a\ R\ z$ and $z\ R^*\ b$.
        By the inductive hypothesis (IH) we have $z\ R^*\ c$ and by the transitive rule we have $a\ R^*\ c$ as desired.
    \end{proofcase}
\end{proof}

Two terms are \textit{convertible}, written $t_1 \equiv t_2$, if $\exists\ t^\prime$ such that $t_1 \betastar t^\prime$ and $t_2 \betastar t^\prime$.
Note that this is not the only way to define convertibility in a type theory, but it is the standard method for a PTS.
Convertibility is used in the typing rules to allow syntax forms to have continued valid types as terms reduce.
It may be tempting to view conversion as the reflexive-symmetric-transitive closure of the step relation, but transitivity is not an obvious property.
In fact, proving transitivity of conversion is often a significant effort, beginning with the confluence lemma.

\begin{lemma}[Confluence]
    If $s \betastar t_1$ and $s \betastar t_2$ then $\exists\ t^\prime$ such that $t_1 \betastar t^\prime$ and $t_2 \betastar t^\prime$
\end{lemma}
\begin{proof}
    See Appendix~\ref{ap:a} for a proof of confluence involving a larger reduction relation. Note that F$^\omega$'s step relation is a subset of this relation and thus is confluent.
\end{proof}

\begin{theorem}[Transitivity of Conversion]
    If $a \equiv b$ and $b \equiv c$ then $a \equiv c$
    \label{thm:1:trans}
\end{theorem}
\begin{proof}
    By premises we know $\exists u, v$ such that $a \betastar u$, $b \betastar u$, $b \betastar v$, and $c \betastar v$.
    By confluence, $\exists z$ such that $u \betastar z$ and $v \betastar z$.
    By transitivity of multistep reduction, $a \betastar z$ and $c \betastar z$.
    Therefore, $a \equiv c$.
\end{proof}

\input{figures/01/infer.tex}

Figure~\ref{fig:typing_f} defines the typing relation on terms for F$^\omega$.
As previously mentioned this formulation is different from standard presentations.
Four relations are defined mutually:
\begin{enumerate}
    \item $\Gamma \vdash t \infr T$, to be read as $T$ is the inferred type of the term $t$ in the context $\Gamma$ or, $t$ infers $T$ in $\Gamma$;
    \item $\Gamma \vdash t \cinfr T$, to be read as $T$ is the inferred type, possibly after some reduction, of the term $t$ in the context $\Gamma$ or, $t$ reduction-infers $T$ in $\Gamma$;
    \item $\Gamma \vdash t \chck T$, to be read as $T$ is checked against the inferred type of the term $t$ in the context $\Gamma$ or, $t$ checks against $T$ in $\Gamma$;
    \item $\vdash \Gamma$, to be read as the context $\Gamma$ is well-formed, and thus consists only of types that themselves have a type
\end{enumerate}
Note that there are two \textsc{Pi} rules that restrict the domain and codomain pairs of function types to three possibilities: $(\kind, \kind)$, $(\star, \star)$, and $(\kind, \star)$.
This is exactly what is required by the $\lambda$-cube for this definition to be F$^\omega$.
For the unfamiliar reading these rules is arcane, thus exposition explaining a small selected set is provided.

$\AxiomRuleF$ The axiom rule has one premise, requiring that the context is well-formed.
It concludes that the constant term $\star$ has type $\kind$.
Intuitively, the term $\star$ should be viewed as a universe of types, or a type of types, often referred to as a \textit{kind}.
Likewise, the term $\kind$ should be viewed as a universe of kinds, or a kind of kinds.
An alternative idea would be to change the conclusion to $\Gamma \vdash \star \infr \star$.
This is called the \textit{type-in-type} rule, and it causes the type theory to be inconsistent \cite{girard1972,hurkens1995}.
Note that there is no way to determine a type for $\kind$.
It plays the role of a type only.

$\VarRuleF$ The variable rule is a context lookup.
It scans the context to determine if the variable is anywhere in context and then the associated type is what that variable infers.
This rule is what requires the typing relation to mention a context.
Whenever a type is inferred or checked it is always desired that the context is well-formed.
That is why the variable rule also requires the context to be well-formed as a premise, because it is a leaf relative to the inference relation.
Without this additional premise there could be typed terms in ill-formed contexts.

$\AppRuleF$ The application rule infers the type of the term $f$ and reduces that type until it looks like a function-type.
Once a function type is required it is clear that the type of the term $a$ must match the function-type's argument-type.
Thus, $a$ is checked against the type $A$.
Finally, the inferred result of the application is the codomain of the function-type $B$ with the term $a$ substituted for any free occurrences of $x$ in $B$.
This substitution is necessary because this application could be a type application to a type function.
For example, let $f = \abs{\lambda}{X}{\star}{\text{id}\ X}$ where id is the identity term.
The inferred type of $f$ is then $(X : \star) \to X \to X$.
Let $a = \mathbb{N}$ (any type constant), then $f\ \mathbb{N} \infr [X := \mathbb{N}](X \to X)$ or $f\ \mathbb{N} \infr \mathbb{N} \to \mathbb{N}$.

While this presentation of F$^\omega$ is not standard Lennon-Bertrand demonstrated that it is equivalent to the standard formulation \cite{lennon2021}.
In fact, Lennon-Bertrand showed that a similar formulation is logically equivalent for the stronger CIC.
Thus, standard metatheoretical results such as preservation and strong normalization still hold.

\begin{lemma}[Preservation of F$^\omega$]
    If $\Gamma \vdash s \chck T$ and $s \betastar t$ then $\Gamma \vdash t \chck T$
\end{lemma}
\begin{proof}
    See Appendix~\ref{ap:b} for a proof of preservation of a conservative extension of F$^\omega$, and thus a proof of preservation for F$^\omega$ itself.
\end{proof}

\begin{theorem}[Strong Normalization of F$^\omega$]
    If $\Gamma \vdash t \infr T$ then $t$ and $T$ are strongly normalizing
\end{theorem}
\begin{proof}
    System F$^\omega$ is a subsystem of CC which has several proofs of strong normalization.
    See (non-exhaustively) proofs using saturated sets \cite{geuvers1994_sn_satset}, model theory \cite{terlouw1995_sn}, realizability \cite{ong1993}, etc.
\end{proof}

With strong normalization the convertibility relation is decidable, and moreover, type checking is decidable.
Let \textit{red} be a function that reduces its input until it is either $\star$, $\kind$, a binder, or in normal form.
Note that this function is defined easily by applying the outermost reduction and matching on the resulting term.
Let \textit{conv} test the convertibility of two terms.
Note that this function may be defined by reducing both terms to normal forms and comparing them for syntactic identity.
Both functions are well-defined because F$^\omega$ is strongly normalizing.
Then the functions \textit{infer}, \textit{check}, and \textit{wf} can be mutually defined by following the typing rules.
Thus, type inference and type checking is decidable for F$^\omega$.

While it is true that F$^\omega$ only has function types as primitives several other data types are internally derivable using function types.
For example, the type of natural numbers is defined:
$$\mathbb{N} = (X : \star) \to X \to (X \to X) \to X$$
Likewise, pairs and sum types are defined:
$$A \times B = (X : \star) \to (A \to B \to X) \to X$$
$$A + B = (X : \star) \to ((A \to X) \times (B \to X)) \to X$$
The logical constants true and false are defined:
$$\top = (X : \star) \to X \to X$$
$$\bot = (X : \star) \to X$$
Negation is defined as implying false:
$$\neg A = A \to \bot$$
These definitions are called \textit{Church encodings} and originate from Church's initial encodings of data in the $\lambda$-calculus \cite{church1932,church1933}.
Note that if there existed a term such that $\vdash t \chck \bot$ then trivially for \textit{any} type $T$ we have $\vdash t\ T \chck T$.
Thus, $\bot$ is both the constant false and the proposition representing the principle of explosion from logic.
Moreover, this allows a concise statement of the consistency of F$^\omega$.

\begin{theorem}[Consistency of System F$^\omega$]
    There is no term $t$ such that $\vdash t \chck \bot$
\end{theorem}
\begin{proof}
    Suppose $\vdash t \chck \bot$.
    Let $n$ be the value of $t$ after it is normalized.
    By preservation $\vdash n \chck \bot$.
    Deconstructing the checking judgment we know that $\vdash n \infr T$ and $T \equiv \bot$, but $\bot$ is a value and values like $n$ infer types that are also values.
    Thus, $T = \bot$ and we know that $\vdash n \infr \bot$.
    By inversion on the typing rules $n = \abs{\lambda}{X}{\star}{b}$, and we have $X : \star \vdash b \infr X$.
    The term $b$ can only be $\star$, $\kind$, or $X$, but none of these options infer type $X$.
    Therefore, there does not exist a term $b$, nor a term $n$, nor a term $t$.
\end{proof}

Recall that induction principles cannot be derived internally for any encoding of data \cite{geuvers2001_noind}.
This is not only cumbersome but unsatisfactory as the natural numbers are in their essence the least set satisfying induction.
Ultimately, the issue is that these encodings are too general.
They admit theoretical elements that F$^\omega$ is not flexible enough to express nor strong enough to exclude.

\section{Calculus of Constructions and Cedille}

As previously mentioned, CC is one extension away from F$^\omega$ on the $\lambda$-cube.
Indeed, the two rules \textsc{Pi1} and \textsc{Pi2} can be merged to form CC:
$$\PiRuleCC$$
where now both $K_1$ and $K_2$ are metavariables representing either $\star$ or $\kind$.
Note that no other rules, syntax, or reductions need to be changed.
Replacing \textsc{Pi1} and \textsc{Pi2} with this new \textsc{Pi} rule is enough to obtain a complete and faithful definition of CC.

With this merger types are allowed to depend on terms.
From a logical point of view, this is a quantification over terms in formula.
Hence, why CC is a predicate logic instead of a propositional one according to the Curry-Howard correspondence.
Yet, there is a question about what exactly quantification over terms means.
Surely it does not mean quantification over syntactic forms.

It means, at minimum, quantification over well-typed terms, but from a logical perspective these terms correspond to proofs.
In first order predicate logic the domain of quantification ranges over a set of \textit{individuals}.
The set of individuals represents any potential set of interest with specific individuals identified through predicates expressing their properties.
With proofs the situation is different.
A proof has meaning relative to its formula, but this meaning may not be relevant as an individual in predicate logic.
For example, the proof $2$ for a Church encoded natural number is intuitively data, but a proof that $2$ is even is intuitively not.
In CC, both are merely proofs that can be quantified over.

Cedille alters the domain of quantification from proofs to (untyped) $\lambda$-caluclus terms.
Thus, for Cedille, the proof $2$ becomes the encoding of $2$ and the proof that $2$ is even can \textit{also} be the encoding of $2$.
This is achieved through a notion of \textit{erasure} which removes type information and auxiliary syntactic forms from a term.
Additionally, convertibility is modified to be convertibility of $\lambda$-calculus terms.
However, erasure as it is defined in Cedille enables diverging terms in inconsistent contexts.
The result by Abel and Coquand, which applies to a wide range of type theories including Cedille, is one way to construct a diverging term \cite{abel2020}.

If terms are able to diverge, in what sense are they a proof?
What a proof is or is not is difficult to say.
As early as Aristotle there are documented forms of argument, Aristotle's syllogisms \cite{aristotle}.
More than a millennium later Euclid's \textit{Elements} is the most well-known example of a mathematical text containing what a modern audience would call proofs.
Moreover, visual renditions of \textit{Elements}, initiated by Byrne, challenge the notion of a proof being an algebraic object \cite{byrne}.
However, the study of proof as a mathematical object dates first to Frege \cite{frege1879} followed soon after by Peano's formalism of arithmetic \cite{peano1889} and Whitehead and Russell's \textit{Principia Mathematica} \cite{whitehead}.
For the kinds of logics discussed by the Curry-Howard correspondence, structural proof theories, the originator is Gentzen \cite{gentzen1935_i,gentzen1935_ii}.
Gentzen's natural deduction describes proofs as finite trees labelled by rules.
Note that this is, of course, a very brief history of mathematical proof.

All of these formulations may be justified as acceptable notions of proof, but the purpose of proof from an epistemological perspective is to provide justification.
It is unsatisfactory to have a claimed proof and be unable to check that it is constructed only by the rules of the proof theory.
This is the situation with Cedille, although rare, there are terms where reduction diverges making it impossible to check a type.
However, it is unfair to levy this criticism against Cedille alone, as well-known type theories also lack decidability of type checking.
For example, Nuprl with its equality reflection rule \cite{allen2000}, and the proof assistant Lean with its notion of casts \cite{moura2021}.
Moreover, Lean has been incredibly successful in formalizing research mathematics including the Liquid Tensor Experiment \cite{liquid_tensor_experiment} and Tao's formalization of The Polynomial Freiman-Ruzsa Conjecture \cite{tao2024_pfr}.
Indeed, not having decidability of type checking does to necessarily prevent a tool from producing convincing arguments.

Ultimately, the definition of proof is a philosophical one with no absolute answer, but this work will follow Gentzen and Kreisel in requiring that a proof is a finite tree, labelled by rules, supporting decidable proof checking.
The reader need only asks themselves which proof they would prefer if the option was available: one that potentially diverges, or one that definitely does not.
If it is the latter, then striving for decidable type theories that are capable enough to reproduce the results obtained by proof assistants like Lean is a worthy goal.

\section{Thesis}

Cedille is a powerful type theory capable of deriving inductive data with relatively modest extension and modification to CC.
However, this capability comes at the cost of decidability of type checking and thus, in the opinion of Kreisel, the cost of a Curry-Howard correspondence to a proof theory.
A redesign of Cedille that focuses on maintaining a proof-theoretic view recovers decidability of type checking while still solving the original goals of Cedille.
Although this redesign does prevent some constructions from being possible, the new balance struck between capability and complexity is desirable because of a well-behaved metatheory.

\section{Contributions}

\textbf{Chapter 2}\quad defines the Cedille2 Core (CC2) theory, including its syntax, and typing rules.
Erasure from Cedille is rephrased as a projection from proofs to objects.
Basic metatheoretical results are proven including: confluence, preservation, and classification.

\textbf{Chapter 3}\quad models CC2 in F$^\omega$ obtaining a strong normalization result for proof normalization.
This model is a straightforward extension of a similar model for CC.
Critically, proof normalization is not powerful enough to show consistency nor object normalization.
Additionally, CC2 is shown to be a conservative extension of F$^\omega$.

\textbf{Chapter 4}\quad models CC2 in CDLE obtaining consistency for CC2.
Although CDLE is not strongly normalizing it still possess a realizability model which justifies its logical consistency.
CC2 is closely related to CDLE which makes this models straightforward to accomplish.
Moreover, a selection of axioms added to CC2 is shown to recover much of CDLEs features.

\textbf{Chapter 5}\quad proves object normalization from proof normalization and consistency.
The $\varphi$, or cast, rule is the only difficulty after proof normalization and consistency.
However, any proof can be translated into a new proof that contains no cast rules.
Applying this observation yields an argument to obtain full object normalization.

\textbf{Chapter 6}\quad with normalization for both proofs and objects a well-founded type checker is defined.
This implementation leverages normalization-by-evaluation and other basic techniques like pattern-based unification.
The tool it benchmarked to demonstrate reasonable performance.

\textbf{Chapter 7}\quad contains derivations of generic inductive data, quotient types, large eliminations, constructor subtyping, and inductive-inductive data.
All of these constructions are possible in Cedille but require modest modifications to derive in Cedille2.

\textbf{Chapter 8}\quad concludes with a collection of open conjectures and questions.
Cedille2 at the conclusion of this work is still in its infancy.