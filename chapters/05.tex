\chapter{Object Normalization and \texorpdfstring{$\varphi$}{Phi} the Foil}
\label{chap:5}

Consistency guarantees that the logic and equational theory of $\ced$ is non-trivial.
Proof normalization guarantees that, at least, inference for kinds and types is decidable.
Neither of these properties are strong enough on their own to guarantee decidability of type checking.
To obtain decidability it must be the case that objects are normalizing.
Unfortunately, object normalization does not hold when the \textsc{Cast} rule is used, and it is not clear how to strike a balance between a rule that yields object normalization and simultaneously enables the same derivation power as Cedille.
A proof is \textbf{strict} if it does not use the \textsc{Cast} rule in its derivation.
Strict proofs do have normalizing objects, but the technique used to prove this fact depends on both proof normalization and consistency.
This is suggestive of how difficult a property object normalization is to show.

\section{Normalization for Strict Proofs}

The core observation is that proof reduction in strict proofs upper-bounds reduction in their corresponding objects.
Thus, if a strict object steps, and note that this must be a $\beta$-step, then there is some strict proof such that the original strict proof reduces to it and the erasures match.
There could be many more reductions in the strict proof because syntax forms for equality and intersections are all mostly erased.
However, none of these forms will block a $\beta$-redex because the proof is well-typed.
Note that this property hinges on both proof normalization and equational consistency.
Proof normalization is used to eliminate any extraneous redexes that would otherwise be erased.
Consistency is used to eliminate the $\delta$ case as it could theoretically generate a $\beta$-redex after erasure if the theory was not equationally consistent.
Note that $\varphi$ could also generate a $\beta$-redex after erasure, but this is impossible because the syntax under consideration is strict.

\begin{definition}
    $\Gamma \vdash_{\text{s}} t : A$ iff $\mathcal{D} : \Gamma \vdash t : A$ and the \textnormal{\textsc{Cast}} rule is not used in $\mathcal{D}$
\end{definition}

\begin{lemma}
    \label{lem:5:strict_erase_red_step}
    If $\Gamma \vdash_{\text{s}} s : A$ and $|s| \betared t$ then $\exists\ t^\prime$ such that $s \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$
\end{lemma}
\begin{proof}
    By induction on $\Gamma \vdash_{\text{s}} s : A$.
    The erasure of the \textsc{Ax}, \textsc{Var}, and \textsc{Refl}, cases are values and thus do not reduce.
    The \textsc{Cast} case is impossible because it is intentionally excluded.
    First projection is very similar to second projection case.
    The \textsc{Int} and \textsc{Eq} cases are structural in erasure and are thus very similar to the \textsc{Pi} case.

    $\text{Case: }\begin{array}{c} \PiRule[*] \end{array}$
    \begin{proofcase}
        Have $|(x : A) \to_m B| = (x : |A|) \to_m |B|$.
        Suppose that $|A| \betared t$.
        By the IH applied to $\D{1}$: $\exists\ t^\prime$ such that $A \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Thus, $(x : A) \to_m B \betastar_{\neq 0} (x : t^\prime) \to_m B$ and $|(x : t^\prime) \to_m B| = (x : t) \to_m |B|$.
        The case where a reduction happens in $|B|$ is similar.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \LambdaRule[*] \end{array}$
    \begin{proofcase}
        Suppose $m = 0$.
        Have $|\abs{\lambda_0}{x}{A}{b}| = |b|$ with $|b| \betared t$.
        Applying the IH to $\D{2}$ concludes the case.
        \\ \\
        Suppose that $m = \omega$, note that $m = \tau$ is very similar and thus omitted.
        Have $|\abs{\lambda_\omega}{x}{A}{b}| = \abs{\lambda_\omega}{x}{\diamond}{|b|}$ and $|b| \betared t$.
        Applying the IH to $\D{2}$ yields $\exists\ t^\prime$ such that $b \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $\abs{\lambda_\omega}{x}{A}{b} \betastar_{\neq 0} \abs{\lambda_\omega}{x}{A}{t^\prime}$ and $|\abs{\lambda_\omega}{x}{A}{t^\prime}| = \abs{\lambda_\omega}{x}{\diamond}{t}$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \AppRule[*] \end{array}$
    \begin{proofcase}
        If $m = 0$ then the proof follows by a straightforward application of the IH to $\D{1}$.
        \\ \\
        Suppose that $m = \omega$.
        Let $|f| = \abs{\lambda_\omega}{x}{\diamond}{v}$ and $\app{|f|}{\omega}{|a|} \betared [x := |a|]v$.
        By Theorem~\ref{lem:3:proof_normalization} $f$ is strongly normalizing in proof reduction.
        If $f$ contains a projection redex, promotion redex, or erased application redex then produce $f_i$ by contracting that redex.
        Continue contracting these redexes until none remain, assume $k$ such redexes are contracted, thus $f \betastar f_k$.
        Note that none of these redexes affect the erasure of $f$, thus $|f| = |f_k|$.
        Now $f_k$ has only three possibilities: $f_k = \abs{\lambda_\omega}{x}{A}{b}$, or $f_k = \psi(\prefl(z; Z), a, b; A, P)$, or $f_k = \delta(\prefl(t; A))$.
        The $\varphi$ case is impossible by the restriction of the judgment and by Theorem~\ref{lem:4:eq_consistency} the $\delta$ case is impossible.
        \begin{itemize}
            \item {
                Suppose $f_k = \abs{\lambda_\omega}{x}{A}{b}$.
                Now $\app{f_k}{\omega}{a} \betared [x := a]b$ and $|[x := a]b| = [x := |a|]v$.
            }
            \item {
                Suppose $f_k = \psi(\prefl(z; Z), a, b; A, P)$.
                Now $\app{\psi(\prefl(z; Z), a, b; A, P)}{\omega}{a} \betared a$.
                Note that $|f_k| = |f|$, but $|\psi(\prefl(z; Z), a, b; A, P)| = \abs{\lambda_\omega}{x}{\diamond}{x}$ and $|f| = \abs{\lambda_\omega}{x}{\diamond}{v}$.
                Thus, $v = x$ and $|a| = [x := |a|]v$.
            }
        \end{itemize}
        \vspace{.15in}
        Suppose $m = \omega$ and $|f| \betared t$.
        Note that the case where $|a| \betared t$ is very similar and thus omitted.
        Applying the IH to $\D{1}$ gives $\exists\ t^\prime$ such that $f \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $\app{f}{\omega}{a} \betastar_{\neq 0} \app{t^\prime}{\omega}{a}$ and $|\app{t^\prime}{\omega}{a}| = \app{t}{\omega}{|a|}$.
        \\ \\
        Suppose $m = \tau$ then erasure is structural.
        Thus, a $\beta$-redex is tracked exactly and any structural redexes are very similar to the $m = \omega$ case.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \PairRule[*] \end{array}$
    \begin{proofcase}
        Have $|[t_1, t_2; A]| = |t_1|$ and $|t_1| \betared t$.
        Applying the IH to $\D{1}$ yields $\exists\ t^\prime$ such that $t_1 \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $[t_1, t_2; A] \betastar_{\neq 0} [t^\prime, t_2; A]$ and $|[t^\prime, t_2; A]| = t$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \SecondRule[*] \end{array}$
    \begin{proofcase}
        Have $|b.2| = |b|$ and $|b| \betared t$.
        Applying the IH to $\D{1}$ gives $\exists\ t^\prime$ such that $b \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $b.2 \betastar_{\neq 0} t^\prime.2$ and $|t^\prime.2| = t$.
    \end{proofcase}

    \begin{minipage}{.8\textwidth}$\text{Case: }\begin{array}{c} \SubstRule[*] \end{array}$\end{minipage}
    \begin{proofcase}
        Have $|\psi(e, a, b; A, T)| = |e|$ and $|e| \betared t$.
        Applying the IH to $\D{4}$ yields $\exists\ t^\prime$ such that $e \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $\psi(e, a, b; A, T) \betastar_{\neq 0} \psi(t^\prime, a, b; A, T)$
            and $|\psi(t^\prime, a, b; A, T)| = t$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \PromoteRule[*] \end{array}$
    \begin{proofcase}
        Have $|\vartheta(e, a, b; (x : A) \cap B)| = |e|$ and $|e| \betared t$.
        Applying the IH to $\D{4}$ gives $\exists\ t^\prime$ where $e \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $\vartheta(e, a, b; (x : A) \cap B) \betastar_{\neq 0} \vartheta(t^\prime, a, b; (x : A) \cap B)$
            and $|\vartheta(t^\prime, a, b; (x : A) \cap B)| = t$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \SeparationRule[*] \end{array}$
    \begin{proofcase}
        Have $|\delta(e)| = |e|$ and $|e| \betared t$.
        Applying the IH to $\D{1}$ gives $\exists\ t^\prime$ where $e \betastar_{\neq 0} t^\prime$ and $|t^\prime| = t$.
        Now $\delta(e) \betastar_{\neq 0} \delta(t^\prime)$ and $|\delta(t^\prime)| = t$.
    \end{proofcase}

    $\text{Case: }\begin{array}{c} \ConvRule[*] \end{array}$
    \begin{proofcase}
        Immediate by the IH applied to $\D{2}$.
    \end{proofcase}

\end{proof}

\begin{theorem}[Strict Object Normalization]
    \label{lem:5:strict_object_sn}
    If $\Gamma \vdash_{\text{s}} t : A$ then $|t|$ is strongly normalizing
\end{theorem}
\begin{proof}
    By Theorem~\ref{lem:3:proof_normalization}: $t$ is strongly normalizing wrt proof reduction.
    Let $\partial$ be the maximum length reduction sequence $t$ could take to reach the unique value.
    Suppose w.l.o.g., that $|t|$ contains a redex.
    Contract this redex giving $|t| \betared e_1$.
    By Lemma~\ref{lem:5:strict_erase_red_step}: $\exists\ t_1$ such that $t \betastar_{\neq 0} t_1$ and $|t_1| = e_1$.
    Using preservation of proof reduction: $\Gamma \vdash_{\text{s}} t_1 : A$.
    Let the number of contracted redexes by the reduction $t \betastar_{\neq 0} t_1$ be $k$, then there is a maximum of $\partial - k$ redexes in $t_1$.
    If redexes remain in $e_1$ than the process can be repeated because $t_1$ is a strict proof whose erasure is $e_1$.
    However, eventually the number of steps taken must run out, because $\partial$ is a finite value.
    Thus, the procedure may be repeated as many times as desired, but $e_i$, the value after $i$ iterations of this process, must eventually run out of redexes by Lemma~\ref{lem:5:strict_erase_red_step}.
    Therefore, $|t|$ is strongly normalizing.
\end{proof}

Strong normalization of strict objects leads to an interesting observation.
Recall the definition of conversion: $a \equiv b$ if and only if $\exists\ u, v$ such that $a \betastar u$, $b \betastar v$ and $|u| \betaconv |v|$.
An observant reader may wonder why reduction is allowed after two candidate objects, $|u|$ and $|v|$ are obtained.
In other words, why not merely compare for equality: $|u| = |v|$.
The answer is because $\varphi$ may generate $\beta$-redexes after erasure.
Moreover, $\varphi$ is the \textit{only} syntax form that might generate a redex.
Thus, if $\varphi$ was removed from the system then conversion \textit{could} be defined using equality of objects instead of conversion reduction of objects.
The $\varphi$ form is unique amongst all the other syntax.

Another question that the reader may have is why not represent the reduction of $\varphi$ in the proof system.
It is not known how to make the reduction well-typed, thus several properties of proof reduction would be lost.
The proof witness of a $\varphi(a, b, e)$ form, $b$, is allowed to be as complicated as required to produce the subtype $(x : A) \cap B$.
However, the object, $|a|$, is typed at the super-type $A$.
To type this term in the proof system some notion of subtyping would have to be added directly into the rules.
Yet, it does hint that the $\varphi$ rule is, in some sense, expanding a semantic subtyping relation that is later realized internally via a notion of casts.
It may be fruitful to view the proof-object distinction as being fundamentally related to subtyping.

\section{Observational Equivalence of Objects}

Unfortunately, proofs involving the $\varphi$ form do not have normalizing objects.
While it is not clear how to augment the proof system to enforce normalization it is possible to describe an external condition on proofs that would guarantee object normalization.
The idea is to observe that each $\varphi(a, b, e)$ form has some associated proof witness ($b$) and some object witness ($a$).
Evidence ($e$) is also provided that these two witnesses are equal at type $A$.
If $e$ reduces to a value, then that implies $|a| \betaconv |b|$, which means that $\varphi$ should be normalizing.
However, the evidence produced in a proof need not ever reduce to a value, yet it will still be discarded by the erasure of $\varphi$.

Observational (or contextual) equivalence of objects gives a strong enough claim to transfer the normalization property from one object to another.
Objects being the concept of interest means that contexts need to be well-typed because an object is the erasure of a proof.
To make contexts the inductive structure of syntax is reused with a unique fresh free variable, labelled $h$, that represents a hole.
The variable is unique meaning it occurs only once in the given syntax, but it can be trivially duplicated by an abstraction.
Context structure could be defined inductively, but this methodology allows reuse of erasure and substitution.
\begin{definition}
    A \textbf{context} $\gamma : (\Gamma, A) \to (\Delta, B)$ is a syntactic form with a unique free variable $h$ representing a hole such that if $\Gamma \vdash t : A$ then $\Delta \vdash [h := t]\gamma : B$.
\end{definition}

Observational equivalence is then defined to be logical equivalence of divergence of the associated objects substituted for $h$ in the given context.
There are several possible ways to define observational equivalence including the choice of what counts as an observation.
For the purposes of this chapter divergence is the only observation of interest.
Note that observational equivalence forms an equivalence relation relative to the parameters $\Gamma$ and $A$.

\begin{definition}
    The syntax $a$ and $b$ are \textbf{observationally equivalent} at $A$ in $\Gamma$ (written: $\Gamma \vdash a \approx_A b$) iff
    for any context $\gamma : (\Gamma, A) \to (\varepsilon, \pUnit)$ with unique fresh variable $h$: $|[h := a]\gamma|$ normalizes iff $|[h := b]\gamma|$ normalizes
\end{definition}

\begin{lemma}
    \label{lem:5:obs_refl}
    $\Gamma \vdash a \approx_A a$
\end{lemma}
\begin{proof}
    Immediate by definition.
\end{proof}

\begin{lemma}
    \label{lem:5:obs_sym}
    If $\Gamma \vdash a \approx_A b$ then $\Gamma \vdash b \approx_A a$
\end{lemma}
\begin{proof}
    By definition the stated condition holds via an if-and-only-if.
    Hence, observational equivalence is symmetric.
\end{proof}

\begin{lemma}
    \label{lem:5:obs_trans}
    If $\Gamma \vdash a \approx_A b$ and $\Gamma \vdash b \approx_A c$ then $\Gamma \vdash a \approx_A c$
\end{lemma}
\begin{proof}
    Let $\gamma : (\Gamma, A) \to (\varepsilon, \pUnit)$ be an arbitrary context with unique fresh variable $h$.
    Suppose $|[h := b]\gamma|$ diverges, then by $\Gamma \vdash b \approx_A c$ it must be the case that $|[h := c]\gamma|$ diverges.
    By Lemma~\ref{lem:5:obs_sym}: $\Gamma \vdash b \approx_A a$ and thus as above $|[h := a]\gamma|$ diverges.
    Suppose $|[h := b]\gamma|$ normalizes, then by $\Gamma \vdash b \approx_A c$: $|[h := c]\gamma|$ normalizes.
    Likewise, using symmetry and the same reasoning: $|[h := a]\gamma|$ normalizes.
    Hence, $|[h := a]\gamma|$ normalizes if and only if $|[h := c]\gamma|$ normalizes.
\end{proof}

\begin{definition}
    A proof is $\varphi$-\textbf{safe} iff for every usage of $\varphi$ with $\Gamma \vdash \varphi(a, b, e) : (x : A) \cap B$ then $\Gamma \vdash \varphi(a, b, e) \approx_{(x : A) \cap B} b$
\end{definition}

\begin{theorem}
    If $\Gamma \vdash t : A$ and $t$ is $\varphi$-safe then $|t|$ is strongly normalizing
\end{theorem}
\begin{proof}
    By lexicographic induction on the nesting count of $\varphi$ in $t$ and the inference judgment $\Gamma \vdash t : A$.
    If $t$ does not contain any $\varphi$ subexpressions then it is a strict proof and thus $|t|$ is strongly normalizing by Theorem~\ref{lem:5:strict_object_sn}.
    Thus, suppose $t$ has $i + 1$ nested $\varphi$ expressions.
    For every case except the \textsc{App} case $|t|$ is strongly normalizing by the IH.
    The \textsc{App} case is special because the function-part could be a $\varphi$ and thus generate a $\beta$-redex in erasure that is not tracked by proof reduction.

    $\text{Case: }\begin{array}{c} \AppRule[*] \end{array}$
    \begin{proofcase}
        Suppose w.l.o.g., that $f = \varphi(a^\prime, b, e).2$ and thus $\app{|f|}{\omega}{|a|} = \app{|a^\prime|}{\omega}{|a|}$.
        By the IH both $|a^\prime|$ and $|a|$ are strongly normalizing.
        Note that $t$ is $\varphi$-safe thus $\Gamma \vdash \varphi(a^\prime, b, e) \approx_{(x : A) \cap B} b$.
        Thus, it must be the case that $\Gamma \vdash \app{\varphi(a^\prime, b, e).2}{\omega}{a} \approx_{[x := a]B} \app{b.2}{\omega}{a}$.
        Hence, for context $\gamma$ with hole $h$: $[h := \app{|a^\prime|}{\omega}{|a|}]|\gamma|$ is normalizing if and only if $[h := \app{|b|}{\omega}{|a|}]|\gamma|$ is normalizing.
        However, $\app{b.2}{\omega}{a}$ has a smaller nesting level of $\varphi$ expressions, thus $\app{|b|}{\omega}{|a|}$ is strongly normalizing.
    \end{proofcase}
\end{proof}

Characterizing when $\varphi$ does not introduce diverging objects is useful because it enables, at the bare minimum, an external validation of each usage.
It is not clear how this requirement may be internalized in the system.
First, a logical relation capturing observational equivalence would likely need to be developed, but because this relation needs to capture equivalence of objects it is not obvious how to adapt existing approaches.
Moreover, the logical relation would have to be bolted on as an auxiliary proof system in order to prove $\varphi$-safety.
At least, the evidence required to use a \textsc{Cast} rule is a sanity check.
If this evidence is ``morally true'' then contextual equivalence will hold by Leibniz's Law.

\begin{conjecture}
    $\Gamma \vdash \varphi(a, b, e) \approx_{(x : A) \cap B} b$ iff $\Gamma \vdash a \approx_{A} b.1$
\end{conjecture}

While the evidence for $\varphi(a, b, e)$ has the type $e : a =_A b.1$ it is clear how to use this evidence to construct a proof $e^\prime : \varphi(a, b, e) =_{(x : A) \cap B} b$.
Eliminate $e$ using $\psi$ and the objects will match.
Going the opposite direction is also clear, as $b$ may be substituted with the left-hand side and the objects will again be identical.
However, it is not clear that a first projection expressed via observational equivalence implies $\varphi$-safety.
The primary obstacle is determining if the erasure of every $\gamma : ((x : A) \cap B, \Gamma) \to (\varepsilon, \pUnit)$ context can be computed via a first projection operation on contexts to obtain $\gamma.1 : (A, \Gamma) \to (\varepsilon, \pUnit)$ with the same erasure.
Demonstrating this conjecture holds would be the first important step to defining a logical relation for contextual equivalence, because it would mean that $\varphi$ terms could be removed entirely from the definition.

\section{Counterexamples with \texorpdfstring{$\varphi$}{Phi}}

It does not take much effort to produce an example of divergence using $\varphi$.
Note, however, that all examples require a context where $\pFalse$ is derivable.
The first example uses $\varphi$ to give self a recursive type: $\pself : \pUnit$ and $\pself : \pUnit \to_\omega \pUnit$ simultaneously.
Divergence is a trivial consequence.
In this example, the False premise is erased, but note that conversion requires reduction under binders regardless.
\begin{align*}
    \pFalse &= (X : \star) \to_0 X_\square \\
    \pself &= \abs{\lambda_\omega}{x}{\pUnit}{
        \apptwo{x}{0}{\pUnit}{\omega}{x}
    } \\
    |\pself| &= \abs{\lambda_\omega}{x}{\diamond}{\app{x}{\omega}{x}} \\
    b &= \abs{\lambda_\omega}{f}{\pFalse}{
        [
            \app{f}{0}{(\pUnit \to_\omega \pUnit)},
            \app{f}{0}{\pUnit}
        ]
    } \\
    e &= \abs{\lambda_\omega}{f}{\pFalse}{
        \app{f}{0}{
            (\pself =_{\pUnit \to_\omega \pUnit} (\app{b}{\omega}{f}).1)
        }
    } \\
    bad &= \abs{\lambda_0}{f}{\pFalse}{
        \app{\pself}{\omega}{
            (
                \varphi(\pself, \app{b}{\omega}{f}, \app{e}{\omega}{f})
            ).2
        }
    } \\
    |bad| &= \app{|\pself|}{\omega}{|\pself|}
\end{align*}
What one can learn from the above example is that hypothetical evidence is problematic for using $\varphi$.
Restricting the context is one idea to make all usages $\varphi$-safe.
Unfortunately, the restriction that $FV(|e|)$ is empty is too strong, it prevents all interesting usages because $b.1 \betastar a$ in all cases as a result.
Instead, the context could be \textit{partially} restricted.
For example, suppose $b : (a : A) \to (x : A) \cap B$ and $e : (a : A) \to a_\star =_A (\app{b}{\omega}{a}).1$ with $FV(|e|)$ empty.
With this setup, $e$ depends only on the single input and expresses only the fact that $b$ is extensionally an identity function.
The object witness term $a$ can then be dropped and the object for the $\varphi$ term would be: $|\varphi(b, e)| = \abs{\lambda_\omega}{x}{\diamond}{x}$.
This idea fails as enough of the context may be uncurried into the type of $A$ to construct a divergent term.
\begin{align*}
    A &= (\pUnit \to_\omega \pUnit) \times \pFalse \\
    T &= (A \to_\omega \pUnit \to_\omega \pUnit) \to_\omega (\pUnit \to_\omega \pUnit) \to_\omega \pUnit \\
    b &= \abs{\lambda_\omega}{w}{A}{
        \app{(\app{\psnd}{\omega}{w})}{0}{(A \cap T)}
    } \\
    e &= \abs{\lambda_\omega}{x}{A}{
        \app{(\app{\psnd}{\omega}{x})}{0}{(x =_A (\app{b}{\omega}{x}).1)}
    } \\
    phi &= \abs{\lambda_\omega}{a}{A}{\varphi(a, \app{b}{\omega}{a}, \app{e}{\omega}{a})} \\
    p1 &= \abs{\lambda_\omega}{f}{\pFalse}{\apptwo{\ppair}{\omega}{\pself}{\omega}{f}} \\
    p2 &= \abs{\lambda_\omega}{x}{A}{\app{\pfst}{\omega}{x}} \\
    p3 &= \abs{\lambda_\omega}{f}{\pFalse}{
        \apptwo{(\app{phi}{\omega}{(\app{p1}{\omega}{f})}).2}
            {\omega}{p2}
            {\omega}{\pself}
    } \\
    bad &= \abs{\lambda_\omega}{f}{\pFalse}{
        \apptwo{(\appthr{p3}{\omega}{f}{0}{\pUnit}{\omega}{\punit})}
            {0}{(\pUnit \to_\omega \pUnit)}
            {\omega}{\pself}
    } \\
    |bad| &= \abs{\lambda_\omega}{f}{\diamond}{\app{|\pself|}{\omega}{|\pself|}}
\end{align*}
This counterexample requires a relevant abstraction, but this could likely be avoided by a more sophisticated formulation.
Finding a balance between usability and restriction of the context is difficult, if not simply impossible without radical alterations.

Another option is to remove $\varphi$ altogether from the system.
It is a significant source of complexity because it demands reduction after erasure in the definition of conversion and is the \textit{only} source of divergence in $\ced$.
To contrast, CDLE has the following sources of divergence:
\begin{enumerate}
    \item indices and witnesses of trivial equalities are untyped $\lambda$-calculus terms;
    \item the rewrite rule, $\rho$, effectively allows casts because rewrites are not typed;
    \item the rewrite rule, $\rho$, is erased enabling non-termination when equality of types is expressible;
    \item the separation rule, $\delta$, is erased enabling non-termination in an inconsistent context;
    \item the $\varphi$ rule, for the same reason as $\ced$.
\end{enumerate}
The initial four sources are eliminated by the design of $\ced$, yet the last remains.
Ultimately, the \textsc{Cast} rule is too important to not only the spirit of Cedille but its capability.
Losing $\varphi$, as far as the current research shows, would prevent almost all existing encodings.
The benefits outweigh the consequences.
